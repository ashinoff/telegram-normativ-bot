import os
import logging
import asyncio
import json
import numpy as np
from typing import List, Dict
from dotenv import load_dotenv

from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters
from yandex_cloud_ml_sdk import YCloudML, AsyncYCloudML

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
load_dotenv()

# Telegram
BOT_TOKEN = os.environ.get('BOT_TOKEN')

# Yandex Cloud
FOLDER_ID = os.environ.get('FOLDER_ID')
API_KEY = os.environ.get('YANDEX_API_KEY')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
MODEL = "yandexgpt-lite"
TEMPERATURE = 0.3
MAX_TOKENS = 2000

# ====================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ======================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ====================== –ö–õ–ê–°–° –î–õ–Ø –ü–û–ò–°–ö–ê –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–ú ======================
class DocumentSearch:
    def __init__(self):
        self.sdk = YCloudML(folder_id=FOLDER_ID, auth=API_KEY)
        self.documents = []
        self.embeddings = []
        self.load_documents()
        
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ documents"""
        docs_path = "documents"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø–∞–ø–∫–∞
        if not os.path.exists(docs_path):
            os.makedirs(docs_path)
            logger.warning(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {docs_path}")
            return
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        loaded_files = []
        for filename in os.listdir(docs_path):
            if filename.endswith(('.txt', '.md')):
                try:
                    filepath = os.path.join(docs_path, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                    chunks = self.split_text(content, 1000)
                    
                    for i, chunk in enumerate(chunks):
                        self.documents.append({
                            'id': f"{filename}_{i}",
                            'filename': filename,
                            'content': chunk,
                            'chunk_index': i
                        })
                    loaded_files.append(filename)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {filename}: {e}")
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(loaded_files)}")
        logger.info(f"–í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        
        # –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if self.documents:
            self.create_embeddings()
    
    def split_text(self, text: str, chunk_size: int) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–≤"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_size += len(word) + 1
            if current_size > chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_size = len(word)
            else:
                current_chunk.append(word)
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def create_embeddings(self):
        """–°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.info(f"–°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(self.documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        
        for i, doc in enumerate(self.documents):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º YandexGPT –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
                response = self.sdk.models.text_embeddings(
                    model="text-search-doc/latest",
                    text=doc['content']
                )
                self.embeddings.append(response.embedding)
                
                if i % 10 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(self.documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ embedding –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ {i}: {e}")
                # –ó–∞–≥–ª—É—à–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
                self.embeddings.append([0.0] * 256)
        
        self.embeddings = np.array(self.embeddings)
        logger.info("Embeddings —Å–æ–∑–¥–∞–Ω—ã!")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.documents:
            logger.warning("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_response = self.sdk.models.text_embeddings(
                model="text-search-query/latest",
                text=query
            )
            query_embedding = np.array(query_response.embedding)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = np.dot(self.embeddings, query_embedding)
            norms = np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
            # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            norms[norms == 0] = 1
            similarities = similarities / norms
            
            # –ù–∞—Ö–æ–¥–∏–º top_k —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.3:  # –ü–æ–Ω–∏–∑–∏–ª –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    results.append({
                        'document': self.documents[idx],
                        'score': float(similarities[idx])
                    })
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

# ====================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ======================
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è YandexGPT
sdk_async = AsyncYCloudML(folder_id=FOLDER_ID, auth=API_KEY)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞
searcher = DocumentSearch()

# ====================== –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –ö–û–ú–ê–ù–î ======================
async def start(update: Update, context):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    docs_count = len(searcher.documents)
    files_count = len(set(doc['filename'] for doc in searcher.documents))
    
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –ü–ü–†–§ 442.\n\n"
        "üìö –ú–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ:\n"
        "‚Ä¢ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–æ–º—É —É—á–µ—Ç—É —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏\n"
        "‚Ä¢ –£—Å—Ç–∞–Ω–æ–≤–∫–µ –∏ –∑–∞–º–µ–Ω–µ –ø—Ä–∏–±–æ—Ä–æ–≤ —É—á–µ—Ç–∞\n"
        "‚Ä¢ –°—Ä–æ–∫–∞–º –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏\n"
        "‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∫–∞–º –∏ –∫–æ–Ω—Ç—Ä–æ–ª—é\n"
        "‚Ä¢ –ë–µ–∑—É—á–µ—Ç–Ω–æ–º—É –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—é\n\n"
        f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {files_count} —Ñ–∞–π–ª–æ–≤ ({docs_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)\n\n"
        "‚ùì –ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å!"
    )

async def list_docs(update: Update, context):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not searcher.documents:
        await update.message.reply_text("üìÇ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
        return
    
    files = set(doc['filename'] for doc in searcher.documents)
    file_list = "\n".join([f"‚Ä¢ {filename}" for filename in sorted(files)])
    
    await update.message.reply_text(
        f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:\n\n{file_list}\n\n"
        f"–í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(searcher.documents)}"
    )

async def handle_message(update: Update, context):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_message = update.message.text
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –±–æ—Ç –ø–µ—á–∞—Ç–∞–µ—Ç
    await context.bot.send_chat_action(
        chat_id=update.effective_chat.id, 
        action="typing"
    )
    
    try:
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        search_results = searcher.search(user_message, top_k=5)  # –£–≤–µ–ª–∏—á–∏–ª –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_text = ""
        sources = []
        
        if search_results:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(search_results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞")
            context_parts = []
            
            for result in search_results:
                doc = result['document']
                score = result['score']
                context_parts.append(f"[–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}] –ò–∑ —Ñ–∞–π–ª–∞ {doc['filename']}:\n{doc['content']}")
                sources.append(doc['filename'])
            
            context_text = "\n\n---\n\n".join(context_parts)
            sources = list(set(sources))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è YandexGPT
        system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§ ‚Ññ442 "–û —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö —Ä—ã–Ω–∫–æ–≤ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–æ–π —ç–Ω–µ—Ä–≥–∏–∏".
        
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –ü–ü–†–§ 442, –µ—Å–ª–∏ –æ–Ω–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç: –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏, –≤—ã–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ–≥–æ
5. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –ø—Ä–∞–∫—Ç–∏—á–µ–Ω –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö"""
        
        messages = [{"role": "system", "text": system_prompt}]
        
        if context_text:
            user_prompt = f"""–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

{context_text}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message}

–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
            messages.append({"role": "user", "text": user_prompt})
        else:
            messages.append({
                "role": "user", 
                "text": f"–í–æ–ø—Ä–æ—Å: {user_message}\n\n–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è. –ß—Ç–æ —Ç—ã –º–æ–∂–µ—à—å —Å–∫–∞–∑–∞—Ç—å –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π –æ –ü–ü–†–§ 442?"
            })
        
        # –ó–∞–ø—Ä–æ—Å –∫ YandexGPT
        result = await sdk_async.models.completions(
            model=MODEL,
            messages=messages,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS
        )
        
        response_text = result.alternatives[0].text
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if sources:
            sources_text = "\n\nüìé _–ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + ", ".join(sources) + "_"
            response_text += sources_text
        elif not search_results:
            response_text += "\n\n‚ö†Ô∏è _–û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö, —Ç–∞–∫ –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞_"
        
        await update.message.reply_text(response_text, parse_mode='Markdown')
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}", exc_info=True)
        await update.message.reply_text(
            "üòî –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –ø–æ–∑–∂–µ."
        )

async def reload_docs(update: Update, context):
    """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    await update.message.reply_text("üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—é –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
    
    try:
        global searcher
        searcher = DocumentSearch()
        
        docs_count = len(searcher.documents)
        files_count = len(set(doc['filename'] for doc in searcher.documents))
        
        await update.message.reply_text(
            f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n"
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {files_count} —Ñ–∞–π–ª–æ–≤ ({docs_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        await update.message.reply_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# ====================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ======================
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    if not all([BOT_TOKEN, FOLDER_ID, API_KEY]):
        logger.error("–ù–µ –∑–∞–¥–∞–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
        logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ BOT_TOKEN, FOLDER_ID, YANDEX_API_KEY")
        return
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    application = Application.builder().token(BOT_TOKEN).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("docs", list_docs))
    application.add_handler(CommandHandler("reload", reload_docs))
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    logger.info(f"ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(searcher.documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()
